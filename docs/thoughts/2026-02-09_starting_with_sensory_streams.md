# Thoughts: Starting With Sensory Streams

Status: processed

Processed in:
- `docs/architecture/sensory_stream_tags.md` (ARC-017)
- `docs/architecture/e3.md` (ARC-003)
- `docs/architecture/hippocampal_systems.md` (ARC-007, ARC-018)
- `docs/architecture/e2.md` (ARC-002)
- `docs/architecture/control_plane.md` (ARC-005)
- `docs/architecture/control_plane_signal_map.md` (MECH-004)
- `docs/architecture/trajectory_selection.md` (IMPL-016)

---

Related claims: ARC-017, ARC-003, ARC-007, ARC-018, ARC-002, ARC-005, INV-012, INV-008

1. Starting constraint: what must exist for any viable agent?

We began by asking a deliberately brutal question:

What sensory distinctions are absolutely necessary for a system to remain viable, adaptive, and ethically coherent — if all it can do is predict, compare predictions, and act?

No semantics.
No goals.
No reward.
Just prediction and intervention.

This forced us to identify sensory stream tags that must be present before learning.

⸻

2. Irreducible sensory stream tags

We arrived at a minimal but sufficient partition of experience:

Primary streams

These define the basic structure of existence for the agent.
	•	WORLD — externally caused sensory input
	•	HOMEOSTASIS — internal viability and drift signals
	•	HARM — threat, damage, catastrophic risk
	•	SELF_SENSORY — reafferent sensory consequences of action

Without these, there is:
	•	no self/world distinction,
	•	no notion of danger vs inconvenience,
	•	no grounding for action.

⸻

Secondary (derived but necessary) streams

These are not “senses” but must exist as explicit tagged signals.
	•	PRECISION — confidence / reliability of predictions
	•	TEMPORAL_COHERENCE — continuity across time
	•	VALENCE — vector-valued trajectory desirability (viability, not reward)

These allow:
	•	uncertainty-aware learning,
	•	stable identity over time,
	•	comparison of futures without scalar collapse.

⸻

Action and responsibility streams

This is where the ethical substrate appears.
	•	ACTION — causal intervention capacity
	•	SELF_IMPACT — predicted vs observed consequences of one’s own actions

This loop is the minimal internal substrate of responsibility.

Without SELF_IMPACT:
	•	behaviour can occur,
	•	prediction can occur,
	•	but responsibility exists only externally.

⸻

3. Emergence (and rejection) of reward

We then asked whether reward-like signals already exist.

They do — but only as derived phenomena:
	•	resolution of predicted harm,
	•	stabilisation of homeostasis,
	•	increased future option-volume,
	•	improved self-impact alignment.

Crucially:
	•	they are vector-valued,
	•	contextual,
	•	non-optimised.

When we stress-tested adding a scalar reward channel, it consistently broke:
	•	epistemic humility (precision corruption),
	•	ethical coherence (Goodhart collapse),
	•	responsibility (self-impact bypass).

Architectural conclusion:
Reward must remain emergent and read-only, never a controller.

⸻

4. Mapping streams onto engines (E1 / E2 / E3)

We then grounded the streams in the REE engines.

E1 — Deep predictor (slow, structural)
	•	Learns world structure, slow risks, temporal invariants
	•	Owns slow HOMEOSTASIS, HARM, TEMPORAL_COHERENCE
	•	Learns long-horizon SELF_IMPACT regularities

E2 — Fast predictor (sensorimotor)
	•	Predicts immediate sensory consequences of action
	•	Computes fast SELF_IMPACT error via reafference
	•	Handles urgency, surprise, rapid correction

E3 — Trajectory selector
	•	Applies vetoes and constraints (HARM, HOMEOSTASIS, SELF_IMPACT)
	•	Ranks surviving futures via VALENCE
	•	Sets precision and routing modes

At this point, rollouts were still implicit.

⸻

5. Key architectural commitment: rollouts are hippocampal

You then locked in a critical constraint:

All trajectory rollouts are generated by the hippocampal system.

This clarified everything.

Hippocampus (HPC)
	•	The only place counterfactual futures exist
	•	Constructs sequences:
	•	“If this action, then this, then this…”
	•	Does no ranking, vetoing, or optimisation

Consequences of this commitment
	•	E1, E2, and E3 do not imagine
	•	E3 cannot bias search, only select
	•	Ethical constraints cannot be “optimised around”
	•	DMN-like cognition becomes a precision/routing mode, not a new system

This is both biologically plausible and ethically stabilising.

⸻

6. Final integrated architecture (cleanly stated)

Pipeline:
	1.	Sensory streams arrive already tagged
	2.	E1 maintains deep generative structure and long-term constraints
	3.	E2 runs fast prediction + reafference comparison
	4.	Hippocampus constructs candidate trajectories under supplied constraints
	5.	E3 selects actions by:
	•	vetoing harm,
	•	respecting viability,
	•	enforcing responsibility,
	•	ranking via vector-valued valence

No reward maximisation.
No goal symbols.
No moral rules.

Ethics emerges because:

the system cannot choose futures that destroy coherence, viability, or responsibility without structurally breaking itself.

⸻

7. The core insight (in one sentence)

REE is an architecture where ethics is not added as a rule or reward, but emerges inevitably from how prediction, action, and self-impact are wired together — with imagination separated from control.

That’s where we are now.

This is a stable stopping point: the primitives are set, the engines are cleanly separated, and the hippocampal commitment prevents most known alignment failure modes.

When you’re ready, the next natural steps would be:
	•	formalising hippocampal “trajectory objects”,
	•	mapping cognitive modes (default mode network, vigilance, sleep) onto precision/routing,
	•	or translating this into a minimal executable spec.


⸻

The missing hippocampal role (now explicit)

We already had:
	•	Hippocampus = constructor of counterfactual trajectories
(“If I did X, then Y, then Z…”)

You’ve now added:

Hippocampus also maps viable paths through the world by learning from the divergence between committed action and predicted action outcomes.

That is:
hippocampus learns the topology of action-space under reality, not just imagined futures.

This matters because it distinguishes:
	•	imagined possibility
	•	from learned viability under commitment

⸻

Two hippocampal functions (now properly separated)

1. Counterfactual rollout (imagination)
	•	Input:
	•	Current latent state
	•	Candidate ACTIONs
	•	Constraints from E1
	•	Precision bounds from E3
	•	Output:
	•	Branching trajectories τ₁, τ₂, …

No learning happens here beyond reuse of structure.

⸻

2. Viability path mapping (commitment learning)
	•	Triggered only after ACTION is actually taken
	•	Uses:
	•	Predicted SELF_SENSORY (from E2, pre-action)
	•	Observed SELF_SENSORY (post-action)
	•	Resulting WORLD / HOMEOSTASIS / HARM changes
	•	Learns:
	•	Which sequences of committed actions remain navigable
	•	Where the agent becomes trapped, destabilised, or harmed
	•	Where prediction remains reliable under execution

This is not “reward learning”.
It is learned affordance geometry under commitment.

⸻

Where this sits in the engine loop

Let’s restate the full loop with this locked in.

Before action
	1.	E1 supplies deep constraints (what usually holds)
	2.	E2 supplies fast forward models (what will likely happen immediately)
	3.	Hippocampus constructs counterfactual trajectories
	4.	E3 selects an ACTION under:
	•	HARM veto
	•	HOMEOSTASIS constraints
	•	SELF_IMPACT structural cost
	•	VALENCE ranking

⸻

After action (this is the key addition)
	5.	ACTION is executed
	6.	E2 computes prediction error (SELF_SENSORY mismatch)
	7.	That error is:
	•	not just used for correction
	•	but routed to hippocampus
	8.	Hippocampus updates its viability graph:
	•	This action-from-this-state is:
	•	stable
	•	fragile
	•	misleading
	•	irreversible
	•	or path-closing

In other words:

Hippocampus learns where imagination remains trustworthy after commitment.

⸻

## Possible affected components

- Sensory stream tagging and reward-readonly stance (ARC-017)
- Trajectory selection and commitment (ARC-003)
- Hippocampal rollouts and viability mapping (ARC-007)
- Fast prediction and reafference comparison (ARC-002)
- Control-plane rollout tuning (ARC-005)

Why this is architecturally crucial

1. It prevents “fantasy planning”

A system that only imagines but never updates path viability will repeatedly plan futures it cannot actually traverse.

This is exactly what breaks in:
	•	mania
	•	obsessional planning
	•	some forms of delusion

Your addition fixes that structurally, not diagnostically.

⸻

2. It grounds ethics in experienced constraint

Because:
	•	SELF_IMPACT error feeds hippocampal path learning
	•	actions that externalise harm or destabilise control
→ become path-closing in the learned topology

So the system doesn’t just “know” something is bad —
it learns that certain futures are no longer reachable without damage.

That’s moral residue, but implemented as geometry.

⸻

3. It explains why commitment matters

Uncommitted rollouts are cheap and plentiful.
Committed actions are expensive and shape the map.

This makes:
	•	hesitation
	•	caution
	•	regret
	•	learning from mistakes

all natural consequences of the architecture.

⸻

Clean updated statement of hippocampus in REE

The hippocampal system in REE serves two roles:
	1.	Trajectory construction: generating counterfactual futures under constraint.
	2.	Viability mapping: learning which paths through action-space remain navigable after commitment, using predicted vs observed action outcome divergence.

Neither role selects.
Neither role optimises.
Both roles constrain what can sensibly be selected in the future.

⸻

Final integrated insight (one sentence)

Imagination proposes futures; commitment teaches which futures remain reachable — and hippocampus is where that difference is learned.

That’s a very strong, very clean place to be.

If you want, next we can:
	•	formalise the “viability graph” representation,
	•	show how regret and caution fall out without being emotions,
	•	or map specific psychiatric failure modes onto breaks in this post-commitment hippocampal update.

so if the control plane really has the knobs we’ve implied, then it has enough authority to do: harm veto, viability respect, and vector-valued valence pruning. In that case E3 collapses into something much closer to an arbitrator of horizon + commitment, rather than a “planner.”

Here’s the clean way to state it.

Control plane must have these knobs

To do what you describe, the control plane needs to be able to:
	1.	Set veto thresholds and interrupt priority

Hard/soft gating over HARM (and some HOMEOSTASIS “slow veto” cases)
	•	Pre-emption: “stop rollout / stop action / freeze / escape”

	2.	Impose viability constraints as constraints, not preferences

	•	HOMEOSTASIS bounds (drift tolerances, recovery requirements)
	•	Action feasibility constraints (don’t choose trajectories that require impossible precision)

	3.	Route and weight prediction errors via precision

	•	Per-stream PRECISION gains (learning rate, attention)
	•	Arbitration between E1 vs E2 trust

	4.	Constrain hippocampal rollout generation

	•	How many branches (breadth)
	•	How far ahead (depth / horizon)
	•	Which action-primitives are permitted as seeds
	•	Whether to allow “fantasy” vs “tight realism” modes

	5.	Apply vector-valued valence filters

	•	Not “maximise a scalar”
	•	But prune futures that are dominated in the viability-vector sense
(e.g., higher harm risk + worse homeostasis + worse coherence)

If those knobs exist, then yes: the control plane can implement the ethical pruning before anything becomes an executable commitment.

Then what is E3?

Under your framing, E3 becomes essentially three tightly related functions:

1) Horizon management

E3 asks: “How far ahead is the imagined future trustworthy enough to use?”

That’s a function of:
	•	current PRECISION
	•	recent SELF_IMPACT mismatch
	•	current HARM urgency
	•	TEMPORAL_COHERENCE stability
	•	hippocampal viability-map confidence (post-commitment learning)

So E3 is selecting an effective planning horizon, not constructing plans.

2) Commitment selection

Given pruned, admissible candidate trajectories, E3 chooses:
	•	the next committed ACTION (or macro-action)
	•	and how long that commitment is held before reconsideration

This is “commitment policy,” not “goal policy.”

3) Efference copy / held-intent for learning

E3 must “hold what it chose” so the rest of the system can learn responsibility:
	•	store the committed action identity
	•	store the predicted SELF_SENSORY consequences (from E2)
	•	store the expected trajectory envelope (from hippocampus, bounded horizon)
	•	then compare to observed outcomes to generate SELF_IMPACT and update the hippocampal viability map

So yes: E3 becomes the keeper of commitments so that error-stream learning is well-posed.

A compact way to say the architecture now
	•	Hippocampus: generates candidate futures + updates viability maps after commitment.
	•	Control plane: sets gains/thresholds/routing + prunes candidates using vetoes, constraints, and vector-valence dominance.
	•	E3: chooses (i) horizon, (ii) commitment, (iii) maintains “what I meant to do” so self-impact learning can occur.

That’s coherent and minimalist.

The one crucial caveat

For the claim “all that is left for E3…” to be strictly true, the control plane must be allowed to prune before selection (not merely to penalise after). If pruning is only post-hoc, you get classic failure modes (the system still chooses harm-seeking actions and merely “regrets” them).

If pruning is pre-selective, then E3 can indeed be mostly horizon + commitment + efference copy.

⸻

If you want to pin this as an architectural commitment in REE language, the shortest version is:

E3 does not evaluate goodness; it evaluates the reliability of evaluation.
It sets horizon/commitment and preserves intended-action traces for self-impact learning, while the control plane enforces veto/viability/valence pruning.
